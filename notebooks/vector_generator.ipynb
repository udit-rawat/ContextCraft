{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class VectorGenerator:\n",
    "    def __init__(self, model_name: str = 'sentence-transformers/all-mpnet-base-v2', embedding_dir: str = '../embeddings'):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.vectors = {}\n",
    "        self.chunk_map = {}  # Maps vector index to original document and chunk\n",
    "        self.embedding_dir = embedding_dir\n",
    "\n",
    "        # Ensure the embeddings directory exists\n",
    "        os.makedirs(self.embedding_dir, exist_ok=True)\n",
    "\n",
    "    def read_chunks(self, parsed_content_dir: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load chunks from text files in the parsed content directory\"\"\"\n",
    "        documents = {}\n",
    "        for file_name in os.listdir(parsed_content_dir):\n",
    "            if file_name.endswith('.txt'):\n",
    "                with open(os.path.join(parsed_content_dir, file_name), 'r', encoding='utf-8') as f:\n",
    "                    chunk_text = f.read()\n",
    "                # Group by base filename without the chunk suffix\n",
    "                # Remove _chunk_x suffix\n",
    "                base_name = \"_\".join(file_name.split('_')[:-2])\n",
    "                documents.setdefault(base_name, []).append(chunk_text)\n",
    "        return documents\n",
    "\n",
    "    def generate_embeddings(self, documents: Dict[str, List[str]], batch_size: int = 32):\n",
    "        \"\"\"Generate embeddings for all document chunks\"\"\"\n",
    "        current_index = 0\n",
    "\n",
    "        for doc_name, chunks in documents.items():\n",
    "            print(f\"Generating embeddings for {doc_name}\")\n",
    "\n",
    "            # Generate embeddings in batches\n",
    "            for i in range(0, len(chunks), batch_size):\n",
    "                batch = chunks[i:i + batch_size]\n",
    "                # Generate embeddings\n",
    "                with torch.no_grad():\n",
    "                    embeddings = self.model.encode(batch)\n",
    "\n",
    "                # Store embeddings and mapping\n",
    "                for j, embedding in enumerate(embeddings):\n",
    "                    self.vectors[current_index] = embedding\n",
    "                    self.chunk_map[current_index] = {\n",
    "                        'document': doc_name,\n",
    "                        'chunk_index': i + j,\n",
    "                        'text': chunks[i + j]\n",
    "                    }\n",
    "                    current_index += 1\n",
    "\n",
    "            print(f\"Generated {len(chunks)} embeddings for {doc_name}\")\n",
    "\n",
    "    def save_vectors(self):\n",
    "        \"\"\"Save combined vectors and chunk mapping to the embeddings directory\"\"\"\n",
    "        combined_data_path = os.path.join(self.embedding_dir, 'vectors.pkl')\n",
    "        data = {\n",
    "            'vectors': self.vectors,\n",
    "            'chunk_map': self.chunk_map,\n",
    "        }\n",
    "\n",
    "        with open(combined_data_path, 'wb') as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "        print(f\"Saved combined vectors and chunk map to {combined_data_path}\")\n",
    "\n",
    "    def load_vectors(self):\n",
    "        \"\"\"Load combined vectors and chunk mapping from the embeddings directory\"\"\"\n",
    "        combined_data_path = os.path.join(self.embedding_dir, 'vectors.pkl')\n",
    "\n",
    "        with open(combined_data_path, 'rb') as file:\n",
    "            data = pickle.load(file)\n",
    "            self.vectors = data['vectors']\n",
    "            self.chunk_map = data['chunk_map']\n",
    "\n",
    "        print(f\"Loaded vectors and chunk map from {combined_data_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading chunks from parsed content directory...\n",
      "Generating embeddings...\n",
      "Generating embeddings for tsla-20231231-gen\n",
      "Generated 434 embeddings for tsla-20231231-gen\n",
      "Generating embeddings for uber-10-k-2023\n",
      "Generated 683 embeddings for uber-10-k-2023\n",
      "Generating embeddings for goog-10-k-2023 (1)\n",
      "Generated 337 embeddings for goog-10-k-2023 (1)\n",
      "Saving embeddings...\n",
      "Saved combined vectors and chunk map to ../embeddings/vectors.pkl\n",
      "\n",
      "Vector Generation Summary:\n",
      "Total vectors generated: 1454\n",
      "Vector dimension: 768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parsed_content_dir = '../parsed_content'\n",
    "    embedding_dir = '../embeddings'\n",
    "\n",
    "    # Initialize the vector generator\n",
    "    vector_gen = VectorGenerator(embedding_dir=embedding_dir)\n",
    "\n",
    "    # Load chunks\n",
    "    print(\"Loading chunks from parsed content directory...\")\n",
    "    documents = vector_gen.read_chunks(parsed_content_dir)\n",
    "\n",
    "    # Generate vectors\n",
    "    print(\"Generating embeddings...\")\n",
    "    vector_gen.generate_embeddings(documents)\n",
    "\n",
    "    # Save vectors\n",
    "    print(\"Saving embeddings...\")\n",
    "    vector_gen.save_vectors()\n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nVector Generation Summary:\")\n",
    "    print(f\"Total vectors generated: {len(vector_gen.vectors)}\")\n",
    "    print(f\"Vector dimension: {len(next(iter(vector_gen.vectors.values())))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content_engine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
