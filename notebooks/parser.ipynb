{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf\n",
    "import os\n",
    "from typing import List, Dict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentParser:\n",
    "    def __init__(self, pdf_directory: str, output_directory: str):\n",
    "        self.pdf_directory = pdf_directory\n",
    "        self.output_directory = output_directory\n",
    "        self.documents = {}\n",
    "\n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(self.output_directory, exist_ok=True)\n",
    "\n",
    "    def read_pdf(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF file\"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = pypdf.PdfReader(file)\n",
    "            text = ''\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + '\\n'\n",
    "        return text\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean extracted text\"\"\"\n",
    "        # Remove multiple newlines\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'[^\\w\\s\\.,!?-]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def split_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:\n",
    "        \"\"\"Split text into chunks of approximately equal size\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "\n",
    "        for word in words:\n",
    "            current_size += len(word) + 1  # +1 for space\n",
    "            if current_size > chunk_size:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [word]\n",
    "                current_size = len(word)\n",
    "            else:\n",
    "                current_chunk.append(word)\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def save_chunks(self, filename: str, chunks: List[str]):\n",
    "        \"\"\"Save chunks into the output directory as text files\"\"\"\n",
    "        base_filename = os.path.splitext(filename)[0]\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_file_path = os.path.join(\n",
    "                self.output_directory, f\"{base_filename}_chunk_{i + 1}.txt\")\n",
    "            with open(chunk_file_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(chunk)\n",
    "\n",
    "    def process_documents(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Process all PDF documents in the directory\"\"\"\n",
    "        for filename in os.listdir(self.pdf_directory):\n",
    "            if filename.endswith('.pdf'):\n",
    "                file_path = os.path.join(self.pdf_directory, filename)\n",
    "                # Extract text\n",
    "                raw_text = self.read_pdf(file_path)\n",
    "                # Clean text\n",
    "                cleaned_text = self.clean_text(raw_text)\n",
    "                # Split into chunks\n",
    "                chunks = self.split_into_chunks(cleaned_text)\n",
    "                # Save chunks to output directory\n",
    "                self.save_chunks(filename, chunks)\n",
    "                # Store with filename as key\n",
    "                self.documents[filename] = chunks\n",
    "\n",
    "        return self.documents\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Processing Summary:\n",
      "\n",
      "Document: goog-10-k-2023 (1).pdf\n",
      "Number of chunks: 337\n",
      "Average chunk length: 996.39 characters\n",
      "Sample chunk: UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549 ___________________________________________ FORM 10-K ___________________________________________ Mark One ANNUAL REPORT PURSUAN...\n",
      "\n",
      "Document: tsla-20231231-gen.pdf\n",
      "Number of chunks: 434\n",
      "Average chunk length: 994.44 characters\n",
      "Sample chunk: UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549 FORM 10-K Mark One x ANNUAL REPORT PURSUANT TO SECTION 13 OR 15d OF THE SECURITIES EXCHANGE ACT OF 1934 For the fiscal year ende...\n",
      "\n",
      "Document: uber-10-k-2023.pdf\n",
      "Number of chunks: 683\n",
      "Average chunk length: 995.01 characters\n",
      "Sample chunk: UNITED STATESSECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549____________________________________________ FORM 10-K ____________________________________________ Mark One ANNUAL REPORT PURSUAN...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Paths for input and output directories\n",
    "    pdf_directory = '../pdfs'\n",
    "    output_directory = '../parsed_content'\n",
    "\n",
    "    parser = DocumentParser(pdf_directory, output_directory)\n",
    "    documents = parser.process_documents()\n",
    "\n",
    "    # Print some statistics\n",
    "    print(\"\\nDocument Processing Summary:\")\n",
    "    for doc_name, chunks in documents.items():\n",
    "        print(f\"\\nDocument: {doc_name}\")\n",
    "        print(f\"Number of chunks: {len(chunks)}\")\n",
    "        print(\n",
    "            f\"Average chunk length: {sum(len(chunk) for chunk in chunks)/len(chunks):.2f} characters\")\n",
    "        print(f\"Sample chunk: {chunks[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "content_engine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
